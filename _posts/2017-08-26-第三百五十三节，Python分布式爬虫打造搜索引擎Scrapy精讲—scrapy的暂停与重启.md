
---
layout: post
title: " 第三百五十三节，Python分布式爬虫打造搜索引擎Scrapy精讲—scrapy的暂停与重启 "
author: "Ans1ble"
header-style: text
tags:
      - Python
---


**第三百五十三节，Python分布式爬虫打造搜索引擎Scrapy精讲—scrapy的暂停与重启**



****scrapy的每一个爬虫， **暂停时可以记录 **暂停状态以及爬取了哪些url，** 重启时可以从 ** ** **
**暂停状态开始爬取过的URL不在爬取**************

**************实现 **暂停与重启记录状态****************

****************1、首先cd进入到 ** **scrapy项目里********************

********************2、在 ** **scrapy项目里创建保存记录信息的文件夹************************

************************![](https://images2017.cnblogs.com/blog/955761/201708/955761-20170826085605589-2071342281.png)************************

************************3、执行命令：************************

************************ scrapy crawl 爬虫名称 -s
JOBDIR=保存记录信息的路径************************

************************如： scrapy crawl cnblogs -s
JOBDIR=zant/001************************

************************执行命令会启动指定爬虫，并且记录状态到指定目录************************

************************![](https://images2017.cnblogs.com/blog/955761/201708/955761-20170826090104449-692340744.png)************************

************************爬虫已经启动，我们可以按键盘上的ctrl+c停止爬虫************************



************************停止后我们看一下记录文件夹，会多出3个文件************************

************************![](https://images2017.cnblogs.com/blog/955761/201708/955761-20170826090703699-894665937.png)************************

************************其中的requests.queue文件夹里的p0文件就是URL记录文件，这个文件存在就说明还有未完成的URL，当所有URL完成后会自动删除此文件************************

************************当我们重新执行命令： ** ** ** ** ** ** ** ** ** ** ** **scrapy
crawl cnblogs -s JOBDIR=zant/001  时爬虫会根据 ** ** ** ** ** ** ** ** ** ** **
**p0文件从停止的地方开始继续爬取，************************************************************************



