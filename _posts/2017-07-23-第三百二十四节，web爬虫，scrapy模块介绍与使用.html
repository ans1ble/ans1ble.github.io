第三百二十四节，web爬虫，scrapy模块介绍与使用


			<div id="cnblogs_post_body" class="blogpost-body"><p><strong>第三百二十四节，web爬虫，scrapy模块介绍与使用</strong></p>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 其可以应用在数据挖掘，信息处理或存储历史数据等一系列的程序中。<br>其最初是为了页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<p>Scrapy 使用了 Twisted异步网络库来处理网络通讯。整体架构大致如下</p>
<p><img src="https://images2015.cnblogs.com/blog/955761/201707/955761-20170723212038159-992324312.png" alt=""></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>Scrapy主要包括了以下组件：</strong></span></p>
<ul>
<li><strong>引擎(Scrapy)</strong><br>用来处理整个系统的数据流处理, 触发事务(框架核心)</li>
<li><strong>调度器(Scheduler)</strong><br>用来接受引擎发过来的请求, 压入队列中, 并在引擎再次请求的时候返回. 可以想像成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么, 同时去除重复的网址</li>
<li><strong>下载器(Downloader)</strong><br>用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的)</li>
<li><strong>爬虫(Spiders)</strong><br>爬虫是主要干活的, 用于从特定的网页中提取自己需要的信息, 即所谓的实体(Item)。用户也可以从中提取出链接,让Scrapy继续抓取下一个页面</li>
<li><strong>项目管道(Pipeline)</strong><br>负责处理爬虫从网页中抽取的实体，主要的功能是持久化实体、验证实体的有效性、清除不需要的信息。当页面被爬虫解析后，将被发送到项目管道，并经过几个特定的次序处理数据。</li>
<li><strong>下载器中间件(Downloader Middlewares)</strong><br>位于Scrapy引擎和下载器之间的框架，主要是处理Scrapy引擎与下载器之间的请求及响应。</li>
<li><strong>爬虫中间件(Spider Middlewares)</strong><br>介于Scrapy引擎和爬虫之间的框架，主要工作是处理蜘蛛的响应输入和请求输出。</li>
<li><strong>调度中间件(Scheduler Middewares)</strong><br>介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。</li>




</ul>
<p>&nbsp;</p>
<p><strong>Scrapy运行流程大概如下：</strong></p>
<ol>
<li>引擎从调度器中取出一个链接(URL)用于接下来的抓取</li>
<li>引擎把URL封装成一个请求(Request)传给下载器</li>
<li>下载器把资源下载下来，并封装成应答包(Response)</li>
<li>爬虫解析Response</li>
<li>解析出实体（Item）,则交给实体管道进行进一步的处理</li>
<li>解析出的是链接（URL）,则把URL交给调度器等待抓取</li>



</ol>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>创建Scrapy框架项目</strong></span></p>
<p><span style="color: #000000"><strong><strong>Scrapy框架项目是有python安装目录里的Scripts文件夹里scrapy.exe文件创建的，所以python安装目录下的<strong><strong>Scripts文件夹要配置到系统环境变量里，才能运行命令生成项目</strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff"><strong><strong><strong><strong>创建项目</strong></strong></strong></strong></span></p>
<p><span style="color: #000000"><strong><strong><strong><strong>首先运行cmd终端，然后cd 进入要创建项目的目录，如：cd H:\py\14</strong></strong></strong></strong></span></p>
<p><span style="color: #000000"><strong><strong><strong><strong>进入要创建项目的目录后执行命令 <span style="color: #ff0000">scrapy startproject 项目名称</span></strong></strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre>scrapy startproject <span style="background-color: #ff99cc">pach1</span></pre>
</div>
<p><strong>项目创建成功</strong></p>
<p><strong><img src="https://images2015.cnblogs.com/blog/955761/201707/955761-20170723214011705-1505671017.png" alt=""></strong></p>
<p>&nbsp;</p>
<p><strong>项目说明</strong></p>
<p>目录结构如下：</p>
<p class="p1"><span class="s1">├── firstCrawler</span></p>
<p class="p1"><span class="s1">│&nbsp;&nbsp; ├── __init__.py</span></p>
<p class="p1"><span class="s1">│&nbsp;&nbsp; ├── items.py</span></p>
<p class="p1"><span class="s1">│&nbsp;&nbsp; ├── middlewares.py</span></p>
<p class="p1"><span class="s1">│&nbsp;&nbsp; ├── pipelines.py</span></p>
<p class="p1"><span class="s1">│&nbsp;&nbsp; ├── settings.py</span></p>
<p class="p1"><span class="s1">│&nbsp;&nbsp; └── spiders</span></p>
<p class="p1"><span class="s1">│&nbsp;&nbsp; &nbsp; &nbsp; └── __init__.py</span></p>
<p class="p1"><span class="s1">└── scrapy.cfg</span></p>
<ul class="simple">
<ul class="simple">
<li><code class="docutils literal">scrapy.cfg</code>: 项目的配置文件</li>
<li><code class="docutils literal">tems.py</code>: 项目中的item文件，用来定义解析对象对应的属性或字段。</li>
<li><code class="docutils literal">pipelines.py</code>:&nbsp;负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库）<a class="replace_word" title="MySQL知识库" href="http://lib.csdn.net/base/mysql" target="_blank"><br></a></li>
<li><code class="docutils literal">settings.py</code>: 项目的设置文件.</li>
<li>spiders：实现自定义爬虫的目录</li>
<li>middlewares.py：Spider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。</li>

</ul>

</ul>
<p><strong><img src="https://images2015.cnblogs.com/blog/955761/201707/955761-20170723220140619-2067789450.png" alt=""></strong></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>创建第一个爬虫</strong></span></p>
<p><span style="color: #0000ff"><strong>创建爬虫文件在spiders文件夹里创建</strong></span></p>
<p><span style="color: #ff0000"><strong>1、创建一个类必须继承scrapy.Spider类，类名称自定义</strong></span></p>
<p><span style="color: #ff0000; background-color: #ffff00"><strong>类里的属性和方法：</strong></span></p>
<p><span style="color: #ff0000"><strong><span style="color: #0000ff">name属性</span>，设置爬虫名称</strong></span><br><span style="color: #ff0000"><strong><span style="color: #0000ff">allowed_domains属性</span>，设置爬取的域名，不带http</strong></span><br><span style="color: #ff0000"><strong><span style="color: #0000ff">start_urls属性</span>，设置爬取的URL，带http</strong></span><br><span style="color: #ff0000"><strong><span style="color: #0000ff">parse()方法</span>，爬取页面后的回调方法，response参数是一个对象，封装了所有的爬取信息</strong></span></p>
<p><span style="color: #ff0000; background-color: #ffff00"><strong><strong>response对象的方法和属性</strong></strong></span></p>
<p><span style="color: #ff0000"><strong><span style="color: #0000ff">response.url</span>获取抓取的rul</strong></span><br><span style="color: #ff0000"><strong><span style="color: #0000ff">response.body</span>获取网页内容字节类型</strong></span><br><span style="color: #ff0000"><strong><span style="color: #0000ff">response.body_as_unicode()</span>获取网站内容字符串类型</strong></span></p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> -*- coding: utf-8 -*-</span>
<span style="color: #0000ff">import</span><span style="color: #000000"> scrapy


</span><span style="color: #0000ff">class</span><span style="color: #000000"> AdcSpider(scrapy.Spider):
    name </span>= <span style="color: #800000">'</span><span style="color: #800000">adc</span><span style="color: #800000">'</span>                                        <span style="color: #008000">#</span><span style="color: #008000">设置爬虫名称</span>
    allowed_domains = [<span style="color: #800000">'</span><span style="color: #800000">www.shaimn.com</span><span style="color: #800000">'</span><span style="color: #000000">]
    start_urls </span>= [<span style="color: #800000">'</span><span style="color: #800000">http://www.shaimn.com/xinggan/</span><span style="color: #800000">'</span><span style="color: #000000">]

    </span><span style="color: #0000ff">def</span><span style="color: #000000"> parse(self, response):
        current_url </span>= response.url                      <span style="color: #008000">#</span><span style="color: #008000">获取抓取的rul</span>
        body = response.body                            <span style="color: #008000">#</span><span style="color: #008000">获取网页内容字节类型</span>
        unicode_body = response.body_as_unicode()       <span style="color: #008000">#</span><span style="color: #008000">获取网站内容字符串类型</span>
        <span style="color: #0000ff">print</span>(unicode_body)</pre>
</div>
<p><span style="color: #ff0000"><strong>爬虫写好后执行爬虫，cd到爬虫目录里执行<span style="color: #0000ff">scrapy crawl adc --nolog</span>命令，说明：<span style="color: #0000ff"><strong>scrapy crawl adc<span style="color: #008080">(<strong><strong>adc表示</strong></strong>爬虫名称)</span> --nolog<span style="color: #008080">(<strong><strong>--nolog表示不显示日志</strong></strong>)</span></strong></span></strong></span></p>
<p><span style="color: #0000ff"><strong><strong>也可以在PyCharm执行命令</strong></strong></span></p>
<p><span style="color: #ff0000"><strong><img src="https://images2015.cnblogs.com/blog/955761/201707/955761-20170724171121590-563261664.png" alt=""></strong></span></p>
<p>&nbsp;</p></div>