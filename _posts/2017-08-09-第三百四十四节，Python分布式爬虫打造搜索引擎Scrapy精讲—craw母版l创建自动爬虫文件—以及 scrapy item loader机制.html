第三百四十四节，Python分布式爬虫打造搜索引擎Scrapy精讲—craw母版l创建自动爬虫文件—以及 scrapy item loader机制


			<div id="cnblogs_post_body" class="blogpost-body"><p><strong>第三百四十四节，Python分布式爬虫打造搜索引擎Scrapy精讲—craw母版l创建自动爬虫文件—以及 scrapy item loader机制</strong></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>用命令创建自动爬虫文件</strong></span></p>
<p><strong>创建爬虫文件是根据scrapy的母版来创建爬虫文件的</strong></p>
<p><strong>scrapy genspider -l&nbsp;&nbsp;查看scrapy创建爬虫文件可用的母版</strong></p>
<p><strong>Available templates:母版说明</strong><br><strong>　　basic　　 
&nbsp; &nbsp;　　创建基础爬虫文件</strong></p>
<p><strong>　　<span style="background-color: #ff99cc">crawl　</span>　　　 
&nbsp; &nbsp;创建自动爬虫文件</strong><br><strong>　　csvfeed　　 
&nbsp; &nbsp; &nbsp;创建爬取csv数据爬虫文件</strong></p>
<p><strong>　　xmlfeed　　　 
&nbsp;创建爬取xml数据爬虫文件</strong></p>
<p><strong>创建一个基础母版爬虫，其他同理</strong></p>
<p><strong>scrapy 
genspider &nbsp;-t &nbsp;母版名称 &nbsp;爬虫文件名称 &nbsp;要爬取的域名&nbsp;创建一个基础母版爬虫，其他同理</strong><br><strong>如：<span style="background-color: #ccffcc">scrapy genspider -t</span> <span style="background-color: #ff99cc">crawl</span> <span style="background-color: #ffff00">lagou www.lagou.com</span></strong></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><span style="background-color: #ffff00">第一步，</span>配置items.py接收数据字段</strong></span></p>
<p><span style="color: #ff0000"><strong><span style="color: #0000ff">default_output_processor = TakeFirst()</span>默认利用ItemLoader类，加载items容器类填充数据，是列表类型，可以通过TakeFirst()方法，获取到列表里的内容</strong></span></p>
<p><span style="color: #ff0000"><strong><span style="color: #0000ff">input_processor = MapCompose(预处理函数)</span>设置数据字段的预处理函数，可以是多个函数</strong></span></p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> -*- coding: utf-8 -*-</span>

<span style="color: #008000">#</span><span style="color: #008000"> Define here the models for your scraped items</span><span style="color: #008000">
#
#</span><span style="color: #008000"> See documentation in:</span><span style="color: #008000">
#</span><span style="color: #008000"> http://doc.scrapy.org/en/latest/topics/items.html</span><span style="color: #008000">
#</span><span style="color: #008000">items.py,文件是专门用于，接收爬虫获取到的数据信息的，就相当于是容器文件</span>

<span style="color: #0000ff">import</span><span style="color: #000000"> scrapy
</span><span style="color: #0000ff">from</span> scrapy.loader.processors <span style="color: #0000ff">import</span><span style="color: #000000"><span style="background-color: #ffff00"> MapCompose</span>,<span style="background-color: #ff99cc">TakeFirst
</span></span><span style="color: #0000ff">from</span> scrapy.loader <span style="color: #0000ff">import</span> <span style="background-color: #ff99cc">ItemLoader</span>                <span style="color: #008000">#</span><span style="color: #008000">导入ItemLoader类也就加载items容器类填充数据</span>


<span style="background-color: #ff99cc"><span style="color: #0000ff">class</span> LagouItemLoader(ItemLoader):</span>                  <span style="color: #008000">#</span><span style="color: #008000">自定义Loader继承ItemLoader类，在爬虫页面调用这个类填充数据到Item类</span>
    <span style="background-color: #ff99cc">default_output_processor = TakeFirst()</span>          <span style="color: #008000">#</span><span style="color: #008000">默认利用ItemLoader类，加载items容器类填充数据，是列表类型，可以通过TakeFirst()方法，获取到列表里的内容</span>

<span style="color: #0000ff; background-color: #00ffff">def</span><span style="background-color: #00ffff"> tianjia(value):</span>                                 <span style="color: #008000">#</span><span style="color: #008000">自定义数据预处理函数</span>
    <span style="background-color: #00ffff"><span style="color: #0000ff">return</span> <span style="color: #800000">'</span><span style="color: #800000">叫卖录音网</span><span style="color: #800000">'</span>+value</span>                        <span style="color: #008000">#</span><span style="color: #008000">将处理后的数据返给Item</span>

<span style="color: #0000ff">class</span> LagouItem(scrapy.Item):                       <span style="color: #008000">#</span><span style="color: #008000">设置爬虫获取到的信息容器类</span>
    title = scrapy.Field(                           <span style="color: #008000">#</span><span style="color: #008000">接收爬虫获取到的title信息</span>
        <span style="background-color: #ffff00">input_processor = MapCompose(<span style="background-color: #00ffff">tianjia</span>)</span>,      <span style="color: #008000">#</span><span style="color: #008000">将数据预处理函数名称传入MapCompose方法里处理，数据预处理函数的形式参数value会自动接收字段title</span>
    )</pre>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>第二步，编写自动爬虫与利用<span style="color: #0000ff">ItemLoader</span>类加载<span style="color: #0000ff">items容器类</span>填充数据</strong></span></p>
<p><span style="color: #ff0000; background-color: #ffff00"><strong>自动爬虫</strong></span><br><span style="color: #ff0000; background-color: #ffff00"><strong><span style="color: #0000ff">Rule()</span>设置爬虫规则</strong></span><br><span style="background-color: #ffff00"><strong>	　　参数：</strong></span><br><span style="background-color: #ffff00"><strong>	　　<span style="color: #ff0000"><span style="color: #0000ff">LinkExtractor()</span>设置url规则</span></strong></span><br><span style="background-color: #ffff00"><strong>	　　callback='回调函数名称'</strong></span><br><span style="background-color: #ffff00"><strong>	　　follow=True 表示在抓取页面继续深入</strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ffff00"><strong><span style="color: #0000ff">LinkExtractor()</span>对爬虫获取到的url做规则判断处理</strong></span><br><span style="background-color: #ffff00"><strong>	　　参数：</strong></span><br><span style="background-color: #ffff00"><strong>	　　allow= r'jobs/' 是一个正则表达式，表示符合这个url格式的，才提取</strong></span><br><span style="background-color: #ffff00"><strong>	　　deny= r'jobs/' 是一个正则表达式，表示符合这个url格式的，不提取抛弃掉，与allow相反</strong></span><br><span style="background-color: #ffff00"><strong>	　　allow_domains= www.lagou.com/ 表示这个域名下的连接才提取</strong></span><br><span style="background-color: #ffff00"><strong>	　　deny_domains= www.lagou.com/ 表示这个域名下的连接不提取抛弃</strong></span><br><span style="background-color: #ffff00"><strong>	　　restrict_xpaths= xpath表达式  表示可以用xpath表达式限定爬虫只提取一个页面指定区域的URL</strong></span><br><span style="background-color: #ffff00"><strong>	　　restrict_css= css选择器，表示可以用css选择器限定爬虫只提取一个页面指定区域的URL</strong></span><br><span style="background-color: #ffff00"><strong>	　　tags= 'a' 表示爬虫通过a标签去寻找url,默认已经设置，默认即可</strong></span><br><span style="background-color: #ffff00"><strong>	　　attrs= 'href'  表示获取到a标签的href属性，默认已经设置，默认即可</strong></span></p>
<hr>
<p>&nbsp;<span style="color: #ff0000; background-color: #ccffcc"><strong>利用自定义<strong>Loader类继承</strong>ItemLoader类，加载items容器类填充数据</strong></span></p>
<p><em id="__mceDel"><span style="color: #ff0000; background-color: #ccffcc"><strong><span style="color: #0000ff">ItemLoader()</span>实例化一个ItemLoader对象来加载items容器类，填充数据，如果是自定义Loader继承的ItemLoader同样的用法</strong></span><br><span style="background-color: #ccffcc"><strong>	　　参数：</strong></span><br><span style="background-color: #ccffcc"><strong>	　　第一个参数：要填充数据的items容器类注意加上括号，</strong></span><br><span style="background-color: #ccffcc"><strong>	　　第二个参数：response</strong></span></em></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ccffcc"><strong>ItemLoader对象下的方法：</strong></span><br><span style="color: #ff0000; background-color: #ccffcc"><strong>	　　<span style="color: #0000ff">add_xpath('字段名称','xpath表达式')</span>方法，用xpath表达式获取数据填充到指定字段</strong></span><br><span style="color: #ff0000; background-color: #ccffcc"><strong>	　　<span style="color: #0000ff">add_css('字段名称','css选择器')</span>方法，用css选择器获取数据填充到指定字段</strong></span><br><span style="color: #ff0000; background-color: #ccffcc"><strong>	　　<span style="color: #0000ff">add_value('字段名称',字符串内容)</span>方法，将指定字符串数据填充到指定字段</strong></span><br><span style="color: #ff0000; background-color: #ccffcc"><strong>	　　<span style="color: #0000ff">load_item()</span>方法无参，将所有数据生成，load_item()方法被yield后数据被填充items容器指定类的各个字段</strong></span></p>
<p><span style="color: #0000ff"><strong>&nbsp;爬虫文件</strong></span></p>
<p>&nbsp;</p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> -*- coding: utf-8 -*-</span>
<span style="color: #0000ff">import</span><span style="color: #000000"> scrapy
</span><span style="background-color: #ffff00"><span style="color: #0000ff">from</span> scrapy.linkextractors <span style="color: #0000ff">import</span><span style="color: #000000"> LinkExtractor
</span><span style="color: #0000ff">from</span> scrapy.spiders <span style="color: #0000ff">import</span><span style="color: #000000"> CrawlSpider, Rule
</span></span><span style="background-color: #ccffcc"><span style="color: #0000ff">from</span> adc.items <span style="color: #0000ff">import</span> LagouItem,LagouItemLoader</span>  <span style="color: #008000">#</span><span style="color: #008000">导入items容器类,和ItemLoader类</span>


<span style="background-color: #ffff00"><span style="color: #0000ff">class</span> LagouSpider(CrawlSpider):                     <span style="color: #008000">#</span><span style="color: #008000">创建爬虫类</span>
    name = <span style="color: #800000">'</span><span style="color: #800000">lagou</span><span style="color: #800000">'</span>                                  <span style="color: #008000">#</span><span style="color: #008000">爬虫名称</span>
    allowed_domains = [<span style="color: #800000">'</span><span style="color: #800000">www.luyin.org</span><span style="color: #800000">'</span>]             <span style="color: #008000">#</span><span style="color: #008000">起始域名</span>
    start_urls = [<span style="color: #800000">'</span><span style="color: #800000">http://www.luyin.org/</span><span style="color: #800000">'</span>]          <span style="color: #008000">#</span><span style="color: #008000">起始url</span>
<span style="color: #000000">
    rules </span>=<span style="color: #000000"> (
        </span><span style="color: #008000">#</span><span style="color: #008000">配置抓取列表页规则</span>
        Rule(LinkExtractor(allow=(<span style="color: #800000">'</span><span style="color: #800000">ggwa/.*</span><span style="color: #800000">'</span>)), follow=<span style="color: #000000">True),

        </span><span style="color: #008000">#</span><span style="color: #008000">配置抓取内容页规则</span>
        Rule(LinkExtractor(allow=(<span style="color: #800000">'</span><span style="color: #800000">post/\d+.html.*</span><span style="color: #800000">'</span>)), callback=<span style="color: #800000">'</span><span style="color: #800000">parse_job</span><span style="color: #800000">'</span>, follow=<span style="color: #000000">True),
    )

    </span></span><span style="color: #0000ff">def</span> parse_job(self, response):                  <span style="color: #008000">#</span><span style="color: #008000">回调函数，注意：因为CrawlS模板的源码创建了parse回调函数，所以切记我们不能创建parse名称的函数</span>
        <span style="color: #008000">#</span><span style="color: #008000">利用ItemLoader类，加载items容器类填充数据</span>
<span style="background-color: #ccffcc">        item_loader = LagouItemLoader(LagouItem(), response=<span style="color: #000000">response)
        item_loader.add_xpath(</span><span style="color: #800000">'</span><span style="color: #800000">title</span><span style="color: #800000">'</span>,<span style="color: #800000">'</span><span style="color: #800000">/html/head/title/text()</span><span style="color: #800000">'</span><span style="color: #000000">)
        article_item </span>=<span style="color: #000000"> item_loader.load_item()

        </span><span style="color: #0000ff">yield</span> article_item</span></pre>
</div>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>items.py文件与爬虫文件的原理图</strong></span></p>
<p><span style="color: #ff0000"><strong><img src="https://images2017.cnblogs.com/blog/955761/201708/955761-20170810195544745-562557430.png" alt=""></strong></span></p>
<p>&nbsp;</p></div>