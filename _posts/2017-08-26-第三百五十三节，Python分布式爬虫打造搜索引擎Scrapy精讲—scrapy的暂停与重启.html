第三百五十三节，Python分布式爬虫打造搜索引擎Scrapy精讲—scrapy的暂停与重启


			<div id="cnblogs_post_body" class="blogpost-body"><p><strong>第三百五十三节，Python分布式爬虫打造搜索引擎Scrapy精讲—scrapy的暂停与重启</strong></p>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong><strong>scrapy的每一个爬虫，<strong>暂停时可以记录<strong>暂停状态以及爬取了哪些url，</strong>重启时可以从<strong><strong><strong><strong>暂停状态开始爬取过的URL不在爬取</strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong>实现<strong>暂停与重启记录状态</strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong>1、首先cd进入到<strong><strong>scrapy项目里</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>2、在<strong><strong>scrapy项目里创建保存记录信息的文件夹</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201708/955761-20170826085605589-2071342281.png" alt=""></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>3、执行命令：</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>　　<span style="color: #0000ff">scrapy crawl 爬虫名称 -s JOBDIR=保存记录信息的路径</span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>　　如：<span style="color: #0000ff">scrapy crawl cnblogs -s JOBDIR=zant/001</span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><span style="color: #0000ff">　　执行命令会启动指定爬虫，并且记录状态到指定目录</span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><span style="color: #0000ff"><img src="https://images2017.cnblogs.com/blog/955761/201708/955761-20170826090104449-692340744.png" alt=""></span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><span style="color: #0000ff">爬虫已经启动，我们可以按键盘上的ctrl+c停止爬虫</span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><span style="color: #0000ff">停止后我们看一下记录文件夹，会多出3个文件</span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><span style="color: #0000ff"><img src="https://images2017.cnblogs.com/blog/955761/201708/955761-20170826090703699-894665937.png" alt=""></span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><span style="color: #0000ff">其中的requests.queue文件夹里的p0文件就是URL记录文件，这个文件存在就说明还有未完成的URL，当所有URL完成后会自动删除此文件</span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><span style="color: #0000ff">当我们重新执行命令：<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapy crawl cnblogs -s JOBDIR=zant/001 &nbsp;时爬虫会根据<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>p0文件从停止的地方开始继续爬取，</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p></div>