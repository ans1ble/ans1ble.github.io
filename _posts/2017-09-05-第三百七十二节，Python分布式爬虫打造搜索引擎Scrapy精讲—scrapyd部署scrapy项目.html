第三百七十二节，Python分布式爬虫打造搜索引擎Scrapy精讲—scrapyd部署scrapy项目


			<div id="cnblogs_post_body" class="blogpost-body"><p><strong>第三百七十二节，Python分布式爬虫打造搜索引擎Scrapy精讲—scrapyd部署scrapy项目</strong></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><strong>scrapyd模块是专门用于部署<strong>scrapy项目的，可以部署和管理<strong><strong><strong>scrapy项目</strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #000000"><strong><strong><strong>下载地址：https://github.com/scrapy/scrapyd</strong></strong></strong></span></p>
<p><span style="color: #000000"><strong><strong><strong>建议安装</strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre>pip3 install scrapyd</pre>
</div>
<p><span style="color: #000000"><strong><strong><strong>首先安装<strong><strong>scrapyd模块，安装后在<span style="color: #0000ff">Python的安装目录下的Scripts文件夹里</span>会生成<span style="color: #ff0000">scrapyd.exe启动文件</span>，如果这个文件存在说明安装成功，我们就可以执行命令了</strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong>启动<strong><strong><strong><strong><strong>scrapyd服务</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre>在命令输入：scrapyd  </pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170905235344569-1402976283.png" alt=""></p>
<p><span style="color: #0000ff"><strong><strong><strong><strong><strong>如图说明启动成功，关闭或者退出命令窗口，<span style="color: #ff0000">因为我们正真的使用是在指定的启动目录下启动服务的</span></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>指定<strong>启动</strong>服务目录<strong><strong>后</strong></strong><strong>启动服务</strong></strong></span></p>
<p><span style="color: #000000"><strong>重新打开命令，<span style="color: #0000ff">cd</span>进入要指定服务的目录后，执行命令<span style="color: #0000ff">scrapyd</span>启动服务</strong></span></p>
<p><span style="color: #ff0000"><strong><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906001602288-406819736.png" alt=""></strong></span></p>
<p><span style="color: #0000ff"><strong>此时可以看到启动目录里生成了dbs目录</strong></span></p>
<p><span style="color: #ff0000"><strong><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906001719663-1485332134.png" alt=""></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong><strong>dbs目录里是空的什么都没有</strong></strong></span></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><strong>此时我们需要安装scrapyd-client模块</strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong>scrapyd-client模块是专门打包<strong><strong>scrapy爬虫项目到<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapyd服务中的</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #000000"><strong><strong>下载目录：https://github.com/scrapy/scrapyd-client</strong></strong></span></p>
<p><span style="color: #000000"><strong><strong>建议安装</strong></strong></span></p>
<div class="cnblogs_code">
<pre>pip3 install scrapyd-client</pre>
</div>
<p><span style="color: #000000"><strong><strong>安装后在Python的安装目录下的<span style="color: #0000ff">Scripts</span>文件夹里会生成<span style="color: #0000ff">scrapyd-deploy无后缀</span>文件，如果有此文件说明安装成功</strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong>重点说明：这个<strong><strong><span style="color: #0000ff">scrapyd-deploy</span>无后缀文件是启动文件，在Linux系统下可以远行，在windows下是不能远行的，所以我们需要编辑一下使其在<strong><strong><strong><strong>windows可以远行</strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;<img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906193530163-344059436.png" alt=""></p>
<p><strong><span style="color: #ff0000">在此目录里新建一个<span style="color: #0000ff">scrapyd-deploy.bat</span>文件，注意名称一定要和<strong><strong>scrapyd-deploy相同，我们编辑这个bat文件<strong><strong><strong><strong>使其在<strong><strong><strong><strong>windows可以远行</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></strong></p>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906194350304-948537913.png" alt=""></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>scrapyd-deploy.bat文件编辑</strong></span></p>
<p>设置python执行文件路径和scrapyd-deploy无后缀文件路径</p>
<div class="cnblogs_code">
<pre><span style="color: #000000">@echo off</span>
<span style="color: #800000">"</span><span style="color: #800000">C:\Users\admin\AppData\Local\Programs\Python\Python35\python.exe</span><span style="color: #800000">"</span> <span style="color: #800000">"</span><span style="color: #800000">C:\Users\admin\AppData\Local\Programs\Python\Python35\Scripts\scrapyd-deploy</span><span style="color: #800000">"</span> %1 %2 %3 %4 %5 %6 %7 %8 %9</pre>
</div>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>scrapyd-deploy.bat文件编辑好后，打开命令窗口cd 到<strong><span style="color: #0000ff">scrapy项目中</span>有<span style="color: #0000ff">scrapy.cfg</span>文件的目录，然后执行<strong><span style="color: #0000ff">scrapyd-deploy</span>命令，看看我们编辑的<strong><span style="color: #0000ff">scrapyd-deploy.bat</span>文件是否可以执行</strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff"><strong><strong>如果下图表示可以执行</strong></strong></span></p>
<p>&nbsp;<img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906203315757-465062445.png" alt=""></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>设置scrapy项目中的scrapy.cfg文件，这个文件就是给<strong><strong><strong><strong>scrapyd-deploy使用的</strong></strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff">&nbsp;<strong>scrapy.cfg文件</strong></span></p>
<p><span style="color: #0000ff"><strong>注意：下面的中文备注不能写在里面，不然会报错，这写的备注只是方便知道怎么设置</strong></span></p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> Automatically created by: scrapy startproject</span><span style="color: #008000">
#
#</span><span style="color: #008000"> For more information about the [deploy] section see:</span><span style="color: #008000">
#</span><span style="color: #008000"> https://scrapyd.readthedocs.org/en/latest/deploy.html</span>
<span style="color: #000000">
[settings]
default </span>=<span style="color: #000000"> adc.settings

<span style="background-color: #ffff00">[deploy:bobby]                      </span></span><span style="background-color: #ffff00"><span style="color: #008000">#</span><span style="color: #008000">设置部署名称bobby</span>
url = http://localhost:6800/        <span style="color: #008000">#</span><span style="color: #008000">开启url</span>
project = adc                       <span style="color: #008000">#</span><span style="color: #008000">项目名称</span></span></pre>
</div>
<p><span style="color: #0000ff"><strong>命令窗口输入：scrapyd-deploy -l &nbsp; &nbsp; 启动服务，可以看到我们设置的部署名称</strong></span></p>
<p><span style="color: #ff0000"><strong><strong>&nbsp;<img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906204530710-768438109.png" alt=""></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong>&nbsp;</strong></strong></span></p>
<p><strong style="color: #ff0000"><strong>开始打包前，执行一个命令：<span style="color: #0000ff">scrapy list &nbsp; ，这个命令执行成功说明可以打包了，如果没执行成功说明还有工作没完成</span></strong></strong></p>
<p><span style="color: #0000ff"><strong style="color: #ff0000"><strong><strong><strong>注意执行&nbsp;<strong><strong>scrapy list &nbsp;命令的时候很有可能出现错误，如果是<strong><strong>python无法找到<strong><strong><strong><strong><strong><strong><strong><strong>scrapy项目，需要在<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapy项目里的settings.py配置文件里设置成python可识别路径</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> 将当前项目的一级目录adc目录添加到python可以识别目录中</span>
BASE_DIR = os.path.dirname(os.path.abspath(os.path.dirname(<span style="color: #800080">__file__</span><span style="color: #000000">)))
sys.path.insert(0, os.path.join(BASE_DIR, </span><span style="color: #800000">'</span><span style="color: #800000">adc</span><span style="color: #800000">'</span>))</pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906222653085-325111114.png" alt=""></p>
<p><span style="color: #0000ff"><strong>如果错误提示，什么远程计算机拒绝，说明你的<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapy项目有链接远程计算机，如链接数据库或者elasticsearch(搜索引擎)之类的，需要先将链接服务器启动</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff">&nbsp;<strong><strong><strong><strong>执行&nbsp;<strong><strong>scrapy list &nbsp;命令返回了爬虫名称说明一切ok了，如下图</strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff"><strong><strong><strong><strong><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170906223321882-1731244839.png" alt=""></strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong style="color: #ff0000"><strong><strong><strong>到此我们就可以开始打包<strong>scrapy项目到<strong>scrapyd了，用命令结合<strong>scrapy项目中的scrapy.cfg文件设置来打包</strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff"><strong style="color: #ff0000"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapy.cfg文件</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> Automatically created by: scrapy startproject</span><span style="color: #008000">
#
#</span><span style="color: #008000"> For more information about the [deploy] section see:</span><span style="color: #008000">
#</span><span style="color: #008000"> https://scrapyd.readthedocs.org/en/latest/deploy.html</span>
<span style="color: #000000">
[settings]
default </span>=<span style="color: #000000"> adc.settings

[deploy:<span style="background-color: #ff99cc">bobby</span>]                      </span><span style="color: #008000">#</span><span style="color: #008000">设置部署名称bobby</span>
url = http://localhost:6800/        <span style="color: #008000">#</span><span style="color: #008000">开启url</span>
project = <span style="background-color: #ff99cc">adc</span>                       <span style="color: #008000">#</span><span style="color: #008000">项目名称</span></pre>
</div>
<p><span style="color: #0000ff"><strong>执行打包命令：&nbsp;scrapyd-deploy 部署名称 -p 项目名称</strong></span></p>
<p><span style="color: #0000ff"><strong>如：scrapyd-deploy <span style="background-color: #ff99cc">bobby</span> -p <span style="background-color: #ff99cc">adc</span></strong></span></p>
<p><span style="color: #0000ff; background-color: #ffffff"><strong>如下显示表示<strong>scrapy项目打包成功</strong></strong></span></p>
<p><span style="color: #0000ff"><strong><span style="background-color: #ffff00"><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907182640476-1712934607.png" alt=""></span></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong>scrapy项目打包成功后说明</strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong>scrapy项目打包成功后会在<strong>scrapyd启动服务的目录生成相应的文件，如下：</strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong>1、会在<strong><span style="color: #0000ff">scrapyd</span>启动服务的目录下的<span style="color: #0000ff">dbs文件夹</span>生成<span style="color: #0000ff"><strong>scrapy项目名称.db</strong></span></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907183643101-421660620.png" alt=""></strong></strong></span></p>
<p><span style="color: #ff0000"><strong>2、会在<span style="background-color: #ffffff; color: #0000ff">scrapyd</span>启动服务的目录下的<span style="color: #0000ff">eggs文件夹</span>生成<strong><strong><strong><strong><strong><span style="color: #0000ff">scrapy项目名称的文件夹</span>，里面是一个<span style="color: #0000ff"><strong>scrapyd-deploy打包生成的名称.egg</strong></span></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907184407741-1629609170.png" alt=""></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong>3、会将<strong><span style="color: #0000ff">scrapy爬虫项目打包</span>，在<strong><strong><span style="color: #0000ff">scrapy项目里</span>会生成两个文件夹，<span style="color: #0000ff">build文件夹</span>和<span style="color: #0000ff">project.egg-info文件夹</span></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><span style="color: #0000ff"><strong><strong><strong><strong>build文件夹里是打包后的爬虫项目，<strong>scrapyd以后远行的就是这个打包后的项目</strong></strong></strong></strong></strong></span></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><span style="color: #0000ff"><strong><strong><strong><strong><strong><strong><strong><strong><strong>project.egg-info文件夹里是打包时的一些配置</strong></strong></strong></strong></strong></strong></strong></strong></strong></span></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>说明：<strong><span style="color: #0000ff">scrapyd-deploy</span>只负责将<strong><span style="color: #0000ff">scrapy爬虫项目打包给</span><strong><span style="color: #0000ff">scrapyd部署</span>，只需要打包一次，打包后，以后的启动爬虫，停止爬虫等<span style="color: #0000ff"><strong>scrapy项目管理由<strong>scrapyd来完成</strong></strong></span></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapyd<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>管理<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapy项目</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>注意：<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>scrapyd<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>管理用的&nbsp;curl 命令，curl命令不支持windows系统，只支持Linux系统，所以在<strong>windows系统下我们用<span style="color: #0000ff">cmder</span>来执行命令</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong>1、远行爬虫，远行指定<strong>scrapy下面的指定爬虫</strong></strong></strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre>curl http://localhost:6800/schedule.json -d project=<span style="background-color: #ff99cc">scrapy项目名称</span> -d spider=<span style="color: #000000"><span style="background-color: #ff99cc">爬虫名称</span>
如：
curl http:</span>//localhost:6800/schedule.json -d project=<span style="background-color: #ff99cc">adc</span> -d spider=<span style="background-color: #ff99cc">lagou</span></pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907192832460-309819801.png" alt=""></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907194434882-1819820825.png" alt=""></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><strong><strong><strong><strong>2、停止爬虫</strong></strong></strong></strong></strong></strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre>curl http://localhost:6800/cancel.json -d project=<span style="background-color: #ff99cc">scrapy项目名称</span> -d job=<span style="color: #000000"><span style="background-color: #ff99cc">远行ID</span>
如：
curl http:</span>//localhost:6800/cancel.json -d project=<span style="background-color: #ff99cc">adc</span> -d job=<span style="background-color: #ff99cc">5454948c93bf11e7af0040167eb10a7b</span></pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907195339647-1850485079.png" alt=""></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907195410476-1973483969.png" alt=""></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong>3、删除scrapy项目</strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong>注意：一般删除<strong>scrapy项目，需要先执行命令停止项目下在远行的爬虫</strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000; background-color: #ffffff"><strong><strong><strong><strong><strong>删除项目后会删除<strong>scrapyd启动服务的目录下的eggs文件夹生成egg文件，需要重新用<strong>scrapyd-deploy打包后才能再次运行</strong></strong></strong></strong></strong></strong></strong></span></p>
<div class="cnblogs_code">
<pre>curl http://localhost:6800/delproject.json -d project=<span style="color: #000000"><span style="background-color: #ff99cc">scrapy项目名称</span>
如果：
curl http:</span>//localhost:6800/delproject.json -d project=<span style="background-color: #ff99cc">adc</span></pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907200352319-1076917567.png" alt=""></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong>4、查看有多少个scrapy项目在api中</strong></span></p>
<div class="cnblogs_code">
<pre>curl http://localhost:6800/listprojects.json</pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907203840194-977118711.png" alt=""></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><strong>5、查看指定的scrapy项目中有多少个爬虫</strong></strong></span></p>
<div class="cnblogs_code">
<pre>curl http://localhost:6800/listspiders.json?project=<span style="color: #000000"><span style="background-color: #ff99cc">scrapy项目名称</span>
如：
curl http:</span>//localhost:6800/listspiders.json?project=<span style="background-color: #ff99cc">adc</span></pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201709/955761-20170907204324226-1076300126.png" alt=""></p>
<p>&nbsp;</p>
<h3>scrapyd支持的API 介绍</h3>
<p>&nbsp;scrapyd支持一系列api，下面用一个py文件来介绍</p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> -*- coding: utf-8 -*-</span>

<span style="color: #0000ff">import</span><span style="color: #000000"> requests
</span><span style="color: #0000ff">import</span><span style="color: #000000"> json 

baseUrl </span>=<span style="color: #800000">'</span><span style="color: #800000">http://127.0.0.1:6800/</span><span style="color: #800000">'</span><span style="color: #000000">
daemUrl </span>=<span style="color: #800000">'</span><span style="color: #800000">http://127.0.0.1:6800/daemonstatus.json</span><span style="color: #800000">'</span><span style="color: #000000">
listproUrl </span>=<span style="color: #800000">'</span><span style="color: #800000">http://127.0.0.1:6800/listprojects.json</span><span style="color: #800000">'</span><span style="color: #000000">
listspdUrl </span>=<span style="color: #800000">'</span><span style="color: #800000">http://127.0.0.1:6800/listspiders.json?project=%s</span><span style="color: #800000">'</span><span style="color: #000000">
listspdvUrl</span>= <span style="color: #800000">'</span><span style="color: #800000">http://127.0.0.1:6800/listversions.json?project=%s</span><span style="color: #800000">'</span><span style="color: #000000">
listjobUrl </span>=<span style="color: #800000">'</span><span style="color: #800000">http://127.0.0.1:6800/listjobs.json?project=%s</span><span style="color: #800000">'</span><span style="color: #000000">
delspdvUrl</span>= <span style="color: #800000">'</span><span style="color: #800000">http://127.0.0.1:6800/delversion.json</span><span style="color: #800000">'</span>

<span style="color: #008000">#</span><span style="color: #008000">http://127.0.0.1:6800/daemonstatus.json</span><span style="color: #008000">
#</span><span style="color: #008000">查看scrapyd服务器运行状态</span>
r=<span style="color: #000000"> requests.get(daemUrl)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">1.stats :\n %s \n\n</span><span style="color: #800000">'</span>  %<span style="color: #000000">r.text  

</span><span style="color: #008000">#</span><span style="color: #008000">http://127.0.0.1:6800/listprojects.json</span><span style="color: #008000">
#</span><span style="color: #008000">获取scrapyd服务器上已经发布的工程列表</span>
r=<span style="color: #000000"> requests.get(listproUrl)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">1.1.listprojects : [%s]\n\n</span><span style="color: #800000">'</span>  %<span style="color: #000000">r.text
</span><span style="color: #0000ff">if</span> len(json.loads(r.text)[<span style="color: #800000">"</span><span style="color: #800000">projects</span><span style="color: #800000">"</span>])&gt;<span style="color: #000000">0 :
    project </span>= json.loads(r.text)[<span style="color: #800000">"</span><span style="color: #800000">projects</span><span style="color: #800000">"</span><span style="color: #000000">][0]

</span><span style="color: #008000">#</span><span style="color: #008000">http://127.0.0.1:6800/listspiders.json?project=myproject</span><span style="color: #008000">
#</span><span style="color: #008000">获取scrapyd服务器上名为myproject的工程下的爬虫清单</span>
listspd=listspd %<span style="color: #000000"> project
r</span>=<span style="color: #000000"> requests.get(listspdUrl)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">2.listspiders : [%s]\n\n</span><span style="color: #800000">'</span>  %<span style="color: #000000">r.text 
</span><span style="color: #0000ff">if</span> json.loads(r.text).has_key(<span style="color: #800000">"</span><span style="color: #800000">spiders</span><span style="color: #800000">"</span>)&gt;<span style="color: #000000">0 :
    spider </span>=json.loads(r.text)[<span style="color: #800000">"</span><span style="color: #800000">spiders</span><span style="color: #800000">"</span><span style="color: #000000">][0]


</span><span style="color: #008000">#</span><span style="color: #008000">http://127.0.0.1:6800/listversions.json?project=myproject</span><span style="color: #008000">
#</span><span style="color: #008000">#获取scrapyd服务器上名为myproject的工程下的各爬虫的版本</span>
listspdvUrl=listspdvUrl %<span style="color: #000000"> project
r </span>=<span style="color: #000000"> requests.get(listspdvUrl)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">3.listversions : [%s]\n\n</span><span style="color: #800000">'</span>  %<span style="color: #000000">rtext 
</span><span style="color: #0000ff">if</span> len(json.loads(r.text)[<span style="color: #800000">"</span><span style="color: #800000">versions</span><span style="color: #800000">"</span>])&gt;<span style="color: #000000">0 :
    version </span>= json.loads(r.text)[<span style="color: #800000">"</span><span style="color: #800000">versions</span><span style="color: #800000">"</span><span style="color: #000000">][0]

</span><span style="color: #008000">#</span><span style="color: #008000">http://127.0.0.1:6800/listjobs.json?project=myproject</span><span style="color: #008000">
#</span><span style="color: #008000">获取scrapyd服务器上的所有任务清单，包括已结束，正在运行的，准备启动的。</span>
listjobUrl=listjobUrl %<span style="color: #000000"> proName
r</span>=<span style="color: #000000">requests.get(listjobUrl)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">4.listjobs : [%s]\n\n</span><span style="color: #800000">'</span>  %<span style="color: #000000">r.text 


</span><span style="color: #008000">#</span><span style="color: #008000">schedule.json</span><span style="color: #008000">
#</span><span style="color: #008000">http://127.0.0.1:6800/schedule.json -d project=myproject -d spider=myspider</span><span style="color: #008000">
#</span><span style="color: #008000">启动scrapyd服务器上myproject工程下的myspider爬虫，使myspider立刻开始运行，注意必须以post方式</span>
schUrl = baseurl + <span style="color: #800000">'</span><span style="color: #800000">schedule.json</span><span style="color: #800000">'</span><span style="color: #000000">
dictdata </span>={ <span style="color: #800000">"</span><span style="color: #800000">project</span><span style="color: #800000">"</span>:project,<span style="color: #800000">"</span><span style="color: #800000">spider</span><span style="color: #800000">"</span><span style="color: #000000">:spider}
r</span>= reqeusts.post(schUrl, json=<span style="color: #000000"> dictdata)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">5.1.delversion : [%s]\n\n</span><span style="color: #800000">'</span>  %<span style="color: #000000">r.text 


</span><span style="color: #008000">#</span><span style="color: #008000">http://127.0.0.1:6800/delversion.json -d project=myproject -d version=r99'</span><span style="color: #008000">
#</span><span style="color: #008000">删除scrapyd服务器上myproject的工程下的版本名为version的爬虫，注意必须以post方式</span>
delverUrl = baseurl + <span style="color: #800000">'</span><span style="color: #800000">delversion.json</span><span style="color: #800000">'</span><span style="color: #000000">
dictdata</span>={<span style="color: #800000">"</span><span style="color: #800000">project</span><span style="color: #800000">"</span>:project ,<span style="color: #800000">"</span><span style="color: #800000">version</span><span style="color: #800000">"</span><span style="color: #000000">: version }
r</span>= reqeusts.post(delverUrl, json=<span style="color: #000000"> dictdata)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">6.1.delversion : [%s]\n\n</span><span style="color: #800000">'</span>  %<span style="color: #000000">r.text 

</span><span style="color: #008000">#</span><span style="color: #008000">http://127.0.0.1:6800/delproject.json -d project=myproject</span><span style="color: #008000">
#</span><span style="color: #008000">删除scrapyd服务器上myproject工程，注意该命令会自动删除该工程下所有的spider，注意必须以post方式</span>
delProUrl = baseurl + <span style="color: #800000">'</span><span style="color: #800000">delproject.json</span><span style="color: #800000">'</span><span style="color: #000000">
dictdata</span>={<span style="color: #800000">"</span><span style="color: #800000">project</span><span style="color: #800000">"</span><span style="color: #000000">:project  }
r</span>= reqeusts.post(delverUrl, json=<span style="color: #000000"> dictdata)
</span><span style="color: #0000ff">print</span> <span style="color: #800000">'</span><span style="color: #800000">6.2.delproject : [%s]\n\n</span><span style="color: #800000">'</span>  %r.text </pre>
</div>
<p>&nbsp;</p>
<h3>总结一下：</h3>
<div>1、获取状态<br>
<div>http://127.0.0.1:6800/daemonstatus.json</div>
<div>2、获取项目列表<br>http://127.0.0.1:6800/listprojects.json</div>

3、获取项目下已发布的爬虫列表<br>
<div>http://127.0.0.1:6800/listspiders.json?project=myproject</div>

4、获取项目下已发布的爬虫版本列表<br>
<div>http://127.0.0.1:6800/listversions.json?project=myproject</div>

5、获取爬虫运行状态<br>
<div>http://127.0.0.1:6800/listjobs.json?project=myproject<br>&nbsp;6、启动服务器上某一爬虫（必须是已发布到服务器的爬虫）<br>http://localhost:6800/schedule.json （post方式，data={"project":myproject,"spider":myspider}）</div>

7、删除某一版本爬虫<br>
<div>http://127.0.0.1:6800/delversion.json&nbsp;（post方式，data={"project":myproject,"version":myversion}）</div>
<div>8、删除某一工程，包括该工程下的各版本爬虫</div>

&nbsp;&nbsp;
<div>http://127.0.0.1:6800/delproject.json（post方式，data={"project":myproject}）</div>
<div>&nbsp;</div>
<div>
<p>&nbsp;到此，基于scrapyd的爬虫发布教程就写完了。</p>
<p>可能有人会说，我直接用scrapy cwal 命令也可以执行爬虫，个人理解用scrapyd服务器管理爬虫，至少有以下几个优势：</p>
<p>1、可以避免爬虫源码被看到。</p>
<p>2、有版本控制。</p>
<p>3、可以远程启动、停止、删除，正是因为这一点，所以scrapyd也是分布式爬虫的解决方案之一。</p>
</div>

</div></div>