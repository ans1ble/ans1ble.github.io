第三百三十四节，web爬虫讲解2—Scrapy框架爬虫—Scrapy爬取百度新闻，爬取Ajax动态生成的信息


			<div id="cnblogs_post_body" class="blogpost-body"><p><strong>第三百三十四节，web爬虫讲解2—Scrapy框架爬虫—Scrapy爬取百度新闻，爬取Ajax动态生成的信息</strong></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><strong>crapy爬取百度新闻，爬取Ajax动态生成的信息，抓取百度新闻首页的新闻rul地址</strong></strong></span></p>
<p><span style="color: #000000"><strong><strong>有多网站，当你浏览器访问时看到的信息，在html源文件里却找不到，由得信息还是滚动条<strong><strong>滚动到对应的位置后才显示信息，那么这种一般都是 js 的&nbsp;<strong>Ajax 动态请求生成的信息</strong></strong></strong></strong></strong></span></p>
<p><span style="color: #000000"><strong><strong><strong><strong><strong>我们以百度新闻为列：</strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong>1、分析网站</strong></strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff"><strong><strong><strong><strong><strong>首先我们浏览器打开百度新闻，在网页中间部分找一条新闻信息</strong></strong></strong></strong></strong></span></p>
<p><span style="color: #ff0000"><strong><strong><strong><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201707/955761-20170730003948582-512985666.png" alt=""></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong>然后查看源码，看看在源码里是否有这条新闻，可以看到源文件里没有这条信息，这种情况爬虫是无法爬取到信息的</strong></span></p>
<p><img src="https://images2017.cnblogs.com/blog/955761/201707/955761-20170730004255300-102114723.png" alt=""></p>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong>那么我们就需要抓包分析了，启动抓包软件和抓包浏览器，前后有说过软件了，就不在说了，此时我们经过抓包看到这条信息是通过<strong><strong><strong><strong><strong>Ajax动态生成的JSON数据，也就是说，当html页面加载完成后才生成的，所有我们在源文件里无法找到，当然爬虫也找不到</strong></strong></strong></strong></strong></strong></span></p>
<p><span style="color: #0000ff"><strong><strong><strong><strong><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201707/955761-20170730005744425-274581575.png" alt=""></strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><strong><span style="color: #0000ff">我们首先将这个JSON数据网址拿出来，到浏览器看看，我们需要的数据是不是全部在里面，此时我们看到这次请求里只有 17条信息，显然我们需要的信息不是完全在里面，还得继续看看其他js包</span></strong></p>
<p><strong><span style="color: #0000ff"><img src="https://images2017.cnblogs.com/blog/955761/201707/955761-20170730010324597-1515301176.png" alt=""></span></strong></p>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong>我们将抓包浏览器滚动条拉到底，以便触发所有js请求，然后在继续找js包，我们将所有js包都找完了再<strong>也</strong>没看到新闻信息的包了</strong></span></p>
<p><img src="https://images2017.cnblogs.com/blog/955761/201707/955761-20170730011647441-1435931401.png" alt=""></p>
<p>&nbsp;</p>
<p><strong><span style="color: #0000ff">那信息就不在js包里了，我们回头在看看其他类型的请求，此时我们看到很多get请求响应的是我们需要的新闻信息，说明只有第一次那个<strong><strong>Ajax请求返回的JSON数据，后面的<strong><strong><strong>Ajax请求返回的都是html类型的字符串数据，</strong></strong></strong></strong></strong></span></strong></p>
<p><strong><span style="color: #0000ff"><strong><strong><strong><strong><strong><img src="https://images2017.cnblogs.com/blog/955761/201707/955761-20170730013345785-1066463303.png" alt=""></strong></strong></strong></strong></strong></span></strong></p>
<p>&nbsp;</p>
<p><strong><span style="color: #0000ff"><strong><strong><strong><strong><strong>我们将<strong><strong><strong>Ajax请求返回的JSON数据的网址和<strong><strong><strong><strong><strong><strong>Ajax请求返回<strong><strong><strong><strong><strong><strong>html类型的字符串数据网址，拿来做一下比较看看是否能找到一定规律，</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></strong></p>
<p><strong><span style="color: #0000ff"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>此时我们可以看到，<strong><strong><strong><strong><strong><strong><strong><strong><strong>JSON数据的网址和<strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>html类型的字符串数据网址是一个请求地址，</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></strong></p>
<p><strong><span style="color: #0000ff"><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>只是请求时传递的参数不一样而已，那么说明无论返回的什么类型的数据，都是在一个请求地址处理的，只是根据不同的传参返回不同类型的数据而已</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></span></strong></p>
<div class="cnblogs_code">
<pre>http://news.baidu.com/widget?id=LocalNews<span style="color: #ff0000">&amp;ajax</span>=json<span style="color: #ff0000">&amp;t</span><span style="color: #000000">=1501348444467   JSON数据的网址

http://news.baidu.com/widget?id=civilnews</span><span style="color: #ff0000">&amp;t</span><span style="color: #000000">=1501348728134        html类型的字符串数据网址

http://news.baidu.com/widget?id=InternationalNews</span><span style="color: #ff0000">&amp;t</span>=1501348728196    html类型的字符串数据网址</pre>
</div>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong>我们可以将html类型的字符串数据网址加上JSON数据的网址参数，那是否会返回JSON数据类型？试一试，果然成功了</strong></span></p>
<div class="cnblogs_code">
<pre>http://news.baidu.com/widget?id=civilnews<span style="color: #ff0000">&amp;ajax</span><span style="color: #000000">=json        将html类型的字符串数据网址加上JSON数据的网址参数

http://news.baidu.com/widget?id=InternationalNews</span><span style="color: #ff0000">&amp;ajax</span>=json    将html类型的字符串数据网址加上JSON数据的网址参数</pre>
</div>
<p><img src="https://images2017.cnblogs.com/blog/955761/201707/955761-20170730014530613-2070451703.png" alt=""></p>
<p>&nbsp;</p>
<p><span style="color: #0000ff"><strong>这下就好办了，找到所有的<strong>html类型的字符串数据网址，按照上面的方法将其转换成<strong>JSON数据的网址，然后循环的去访问转换后的<strong><strong><strong>JSON数据的网址，就可以拿到所有新闻的url地址了</strong></strong></strong></strong></strong></strong></span></p>
<p>&nbsp;</p>
<p><span style="color: #ff0000"><strong><strong>crapy实现</strong></strong></span></p>
<div class="cnblogs_code">
<pre><span style="color: #008000">#</span><span style="color: #008000"> -*- coding: utf-8 -*-</span>
<span style="color: #0000ff">import</span><span style="color: #000000"> scrapy
</span><span style="color: #0000ff">from</span> scrapy.http <span style="color: #0000ff">import</span><span style="color: #000000"> Request,FormRequest
</span><span style="color: #0000ff">import</span><span style="color: #000000"> re
</span><span style="color: #0000ff">import</span><span style="color: #000000"> json
</span><span style="color: #0000ff">from</span> adc.items <span style="color: #0000ff">import</span><span style="color: #000000"> AdcItem
</span><span style="color: #0000ff">from</span> scrapy.selector <span style="color: #0000ff">import</span><span style="color: #000000"> Selector

</span><span style="color: #0000ff">class</span> PachSpider(scrapy.Spider):                            <span style="color: #008000">#</span><span style="color: #008000">定义爬虫类，必须继承scrapy.Spider</span>
    name = <span style="color: #800000">'</span><span style="color: #800000">pach</span><span style="color: #800000">'</span>                                           <span style="color: #008000">#</span><span style="color: #008000">设置爬虫名称</span>
    allowed_domains = [<span style="color: #800000">'</span><span style="color: #800000">news.baidu.com</span><span style="color: #800000">'</span>]                    <span style="color: #008000">#</span><span style="color: #008000">爬取域名</span>
    start_urls = [<span style="color: #800000">'</span><span style="color: #800000">http://news.baidu.com/widget?id=civilnews&amp;ajax=json</span><span style="color: #800000">'</span><span style="color: #000000">]

    qishiurl </span>= [                    <span style="color: #008000">#</span><span style="color: #008000">的到所有页面id</span>
        <span style="color: #800000">'</span><span style="color: #800000">InternationalNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">FinanceNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">EnterNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">SportNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">AutoNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">HouseNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">InternetNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">InternetPlusNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">TechNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">EduNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">GameNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">DiscoveryNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">HealthNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">LadyNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">SocialNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">MilitaryNews</span><span style="color: #800000">'</span><span style="color: #000000">,
        </span><span style="color: #800000">'</span><span style="color: #800000">PicWall</span><span style="color: #800000">'</span><span style="color: #000000">
    ]

    <span style="background-color: #ff99cc">urllieb </span></span><span style="background-color: #ff99cc">=<span style="color: #000000"> []
    </span></span><span style="color: #0000ff">for</span> i <span style="color: #0000ff">in</span> range(0,len(qishiurl)):            <span style="color: #008000">#</span><span style="color: #008000">构造出所有idURL</span>
        kaishi_url = <span style="color: #800000">'</span><span style="color: #800000">http://news.baidu.com/widget?id=</span><span style="color: #800000">'</span> + qishiurl[i] + <span style="color: #800000">'</span><span style="color: #800000">&amp;ajax=json</span><span style="color: #800000">'</span><span style="color: #000000">
        urllieb.append(kaishi_url)
    </span><span style="color: #008000">#</span><span style="color: #008000"> print(urllieb)</span>

    <span style="color: #0000ff">def</span> parse(self, response):                  <span style="color: #008000">#</span><span style="color: #008000">选项所有连接</span>
<span style="background-color: #ff99cc">        <span style="color: #0000ff">for</span> j <span style="color: #0000ff">in</span><span style="color: #000000"> range(0, len(self.urllieb)):
            a </span>= <span style="color: #800000">'</span><span style="color: #800000">正在处理第%s个栏目:url地址是：%s</span><span style="color: #800000">'</span> %<span style="color: #000000"> (j, self.urllieb[j])
            </span><span style="color: #0000ff">yield</span> scrapy.Request(url=self.urllieb[j], callback=self.enxt)     <span style="color: #008000">#</span><span style="color: #008000">每次循环到的url 添加爬虫</span></span>


    <span style="color: #0000ff">def</span><span style="color: #000000"> enxt(self, response):
        neir </span>= response.body.decode(<span style="color: #800000">"</span><span style="color: #800000">utf-8</span><span style="color: #800000">"</span><span style="color: #000000">)
        pat2 </span>= <span style="color: #800000">'</span><span style="color: #800000">"m_url":"(.*?)"</span><span style="color: #800000">'</span><span style="color: #000000">
        url </span>= re.compile(pat2, re.S).findall(neir)      <span style="color: #008000">#</span><span style="color: #008000">通过正则获取爬取页面 的URL</span>
<span style="background-color: #ff99cc">        <span style="color: #0000ff">for</span> k <span style="color: #0000ff">in</span><span style="color: #000000"> range(0,len(url)):
            zf_url </span>=<span style="color: #000000"> url[k]
            url_zf </span>= re.sub(<span style="color: #800000">"</span><span style="color: #800000">\\\/</span><span style="color: #800000">"</span>, <span style="color: #800000">"</span><span style="color: #800000">/</span><span style="color: #800000">"</span><span style="color: #000000">, zf_url)
            pduan </span>= url_zf.find(<span style="color: #800000">'</span><span style="color: #800000">http://</span><span style="color: #800000">'</span><span style="color: #000000">)
            </span><span style="color: #0000ff">if</span> pduan ==<span style="color: #000000"> 0:
                </span><span style="color: #0000ff">print</span>(url_zf)                       <span style="color: #008000">#</span><span style="color: #008000">输出获取到的所有url</span></span></pre>
</div>
<p>&nbsp;</p></div>